# LLM Gateway - Environment Variables
# Copy this file to .env and customize for your deployment

# Provider Settings
LLM_PROVIDER=mock
LLM_MODEL=gpt-4-mock
# OPENAI_API_KEY=sk-...

# Execution Settings
LLM_TIMEOUT_SECONDS=30
LLM_MAX_RETRIES=2

# Cache Settings
CACHE_ENABLED=true
CACHE_TTL_SECONDS=600
CACHE_MAX_ENTRIES=256

# Rate Limiting (leave empty for no limit)
RATE_LIMIT_QPS=
RATE_LIMIT_TPM=

# Observability
PROMPT_VERSION=1.0
LOG_DIR=runs/logs

# Production Example:
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=sk-your-key-here
# RATE_LIMIT_QPS=100
# RATE_LIMIT_TPM=500000
