version: '3.8'

services:
  # LLM Gateway API
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llmops-api
    ports:
      - "8000:8000"
    environment:
      # Provider settings
      - LLM_PROVIDER=${LLM_PROVIDER:-mock}
      - LLM_MODEL=${LLM_MODEL:-gpt-4-mock}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      
      # Execution settings
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-30}
      - LLM_MAX_RETRIES=${LLM_MAX_RETRIES:-2}
      
      # Cache settings
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-600}
      - CACHE_MAX_ENTRIES=${CACHE_MAX_ENTRIES:-256}
      
      # Rate limiting
      - RATE_LIMIT_QPS=${RATE_LIMIT_QPS:-}
      - RATE_LIMIT_TPM=${RATE_LIMIT_TPM:-}
      
      # Observability
      - PROMPT_VERSION=${PROMPT_VERSION:-1.0}
      - LOG_DIR=${LOG_DIR:-runs/logs}
    volumes:
      # Persist logs
      - ./runs/logs:/app/runs/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Streamlit Dashboard
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    container_name: llmops-dashboard
    ports:
      - "8501:8501"
    volumes:
      # Share logs with API
      - ./runs/logs:/app/runs/logs:ro
    depends_on:
      - api
    restart: unless-stopped
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
